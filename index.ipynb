{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonparametric ML Models - Cumulative Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this cumulative lab, you will apply two nonparametric models you have just learned — k-nearest neighbors and decision trees — to the forest cover dataset.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "* Practice identifying and applying appropriate preprocessing steps\n",
    "* Perform an iterative modeling process, starting from a baseline model\n",
    "* Explore multiple model algorithms, and tune their hyperparameters\n",
    "* Practice choosing a final model across multiple model algorithms and evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task: Complete an End-to-End ML Process with Nonparametric Models on the Forest Cover Dataset\n",
    "\n",
    "![line of pine trees](https://curriculum-content.s3.amazonaws.com/data-science/images/trees.jpg)\n",
    "\n",
    "Photo by <a href=\"https://unsplash.com/@michaelbenz?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Michael Benz</a> on <a href=\"/s/photos/forest?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText\">Unsplash</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business and Data Understanding\n",
    "\n",
    "To repeat the previous description:\n",
    "\n",
    "> Here we will be using an adapted version of the forest cover dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/covertype). Each record represents a 30 x 30 meter cell of land within Roosevelt National Forest in northern Colorado, which has been labeled as `Cover_Type` 1 for \"Cottonwood/Willow\" and `Cover_Type` 0 for \"Ponderosa Pine\". (The original dataset contained 7 cover types but we have simplified it.)\n",
    "\n",
    "The task is to predict the `Cover_Type` based on the available cartographic variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>Soil_Type_31</th>\n",
       "      <th>Soil_Type_32</th>\n",
       "      <th>Soil_Type_33</th>\n",
       "      <th>Soil_Type_34</th>\n",
       "      <th>Soil_Type_35</th>\n",
       "      <th>Soil_Type_36</th>\n",
       "      <th>Soil_Type_37</th>\n",
       "      <th>Soil_Type_38</th>\n",
       "      <th>Soil_Type_39</th>\n",
       "      <th>Cover_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2553</td>\n",
       "      <td>235</td>\n",
       "      <td>17</td>\n",
       "      <td>351</td>\n",
       "      <td>95</td>\n",
       "      <td>780</td>\n",
       "      <td>188</td>\n",
       "      <td>253</td>\n",
       "      <td>199</td>\n",
       "      <td>1410</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011</td>\n",
       "      <td>344</td>\n",
       "      <td>17</td>\n",
       "      <td>313</td>\n",
       "      <td>29</td>\n",
       "      <td>404</td>\n",
       "      <td>183</td>\n",
       "      <td>211</td>\n",
       "      <td>164</td>\n",
       "      <td>300</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>391</td>\n",
       "      <td>42</td>\n",
       "      <td>509</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>134</td>\n",
       "      <td>421</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2038</td>\n",
       "      <td>50</td>\n",
       "      <td>17</td>\n",
       "      <td>408</td>\n",
       "      <td>71</td>\n",
       "      <td>474</td>\n",
       "      <td>226</td>\n",
       "      <td>200</td>\n",
       "      <td>102</td>\n",
       "      <td>283</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018</td>\n",
       "      <td>341</td>\n",
       "      <td>27</td>\n",
       "      <td>351</td>\n",
       "      <td>34</td>\n",
       "      <td>390</td>\n",
       "      <td>152</td>\n",
       "      <td>188</td>\n",
       "      <td>168</td>\n",
       "      <td>190</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38496</th>\n",
       "      <td>2396</td>\n",
       "      <td>153</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>108</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>118</td>\n",
       "      <td>837</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38497</th>\n",
       "      <td>2391</td>\n",
       "      <td>152</td>\n",
       "      <td>19</td>\n",
       "      <td>67</td>\n",
       "      <td>12</td>\n",
       "      <td>95</td>\n",
       "      <td>240</td>\n",
       "      <td>237</td>\n",
       "      <td>119</td>\n",
       "      <td>845</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38498</th>\n",
       "      <td>2386</td>\n",
       "      <td>159</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>236</td>\n",
       "      <td>241</td>\n",
       "      <td>130</td>\n",
       "      <td>854</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38499</th>\n",
       "      <td>2384</td>\n",
       "      <td>170</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>5</td>\n",
       "      <td>90</td>\n",
       "      <td>230</td>\n",
       "      <td>245</td>\n",
       "      <td>143</td>\n",
       "      <td>864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38500</th>\n",
       "      <td>2383</td>\n",
       "      <td>165</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>67</td>\n",
       "      <td>231</td>\n",
       "      <td>244</td>\n",
       "      <td>141</td>\n",
       "      <td>875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38501 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0           2553     235     17                               351   \n",
       "1           2011     344     17                               313   \n",
       "2           2022      24     13                               391   \n",
       "3           2038      50     17                               408   \n",
       "4           2018     341     27                               351   \n",
       "...          ...     ...    ...                               ...   \n",
       "38496       2396     153     20                                85   \n",
       "38497       2391     152     19                                67   \n",
       "38498       2386     159     17                                60   \n",
       "38499       2384     170     15                                60   \n",
       "38500       2383     165     13                                60   \n",
       "\n",
       "       Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
       "0                                  95                              780   \n",
       "1                                  29                              404   \n",
       "2                                  42                              509   \n",
       "3                                  71                              474   \n",
       "4                                  34                              390   \n",
       "...                               ...                              ...   \n",
       "38496                              17                              108   \n",
       "38497                              12                               95   \n",
       "38498                               7                               90   \n",
       "38499                               5                               90   \n",
       "38500                               4                               67   \n",
       "\n",
       "       Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0                188             253            199   \n",
       "1                183             211            164   \n",
       "2                212             212            134   \n",
       "3                226             200            102   \n",
       "4                152             188            168   \n",
       "...              ...             ...            ...   \n",
       "38496            240             237            118   \n",
       "38497            240             237            119   \n",
       "38498            236             241            130   \n",
       "38499            230             245            143   \n",
       "38500            231             244            141   \n",
       "\n",
       "       Horizontal_Distance_To_Fire_Points  ...  Soil_Type_31  Soil_Type_32  \\\n",
       "0                                    1410  ...             0             0   \n",
       "1                                     300  ...             0             0   \n",
       "2                                     421  ...             0             0   \n",
       "3                                     283  ...             0             0   \n",
       "4                                     190  ...             0             0   \n",
       "...                                   ...  ...           ...           ...   \n",
       "38496                                 837  ...             0             0   \n",
       "38497                                 845  ...             0             0   \n",
       "38498                                 854  ...             0             0   \n",
       "38499                                 864  ...             0             0   \n",
       "38500                                 875  ...             0             0   \n",
       "\n",
       "       Soil_Type_33  Soil_Type_34  Soil_Type_35  Soil_Type_36  Soil_Type_37  \\\n",
       "0                 0             0             0             0             0   \n",
       "1                 0             0             0             0             0   \n",
       "2                 0             0             0             0             0   \n",
       "3                 0             0             0             0             0   \n",
       "4                 0             0             0             0             0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "38496             0             0             0             0             0   \n",
       "38497             0             0             0             0             0   \n",
       "38498             0             0             0             0             0   \n",
       "38499             0             0             0             0             0   \n",
       "38500             0             0             0             0             0   \n",
       "\n",
       "       Soil_Type_38  Soil_Type_39  Cover_Type  \n",
       "0                 0             0           0  \n",
       "1                 0             0           0  \n",
       "2                 0             0           0  \n",
       "3                 0             0           0  \n",
       "4                 0             0           0  \n",
       "...             ...           ...         ...  \n",
       "38496             0             0           0  \n",
       "38497             0             0           0  \n",
       "38498             0             0           0  \n",
       "38499             0             0           0  \n",
       "38500             0             0           0  \n",
       "\n",
       "[38501 rows x 53 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/forest_cover.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As you can see, we have over 38,000 rows, each with 52 feature columns and 1 target column:\n",
    "\n",
    "> * `Elevation`: Elevation in meters\n",
    "> * `Aspect`: Aspect in degrees azimuth\n",
    "> * `Slope`: Slope in degrees\n",
    "> * `Horizontal_Distance_To_Hydrology`: Horizontal dist to nearest surface water features in meters\n",
    "> * `Vertical_Distance_To_Hydrology`: Vertical dist to nearest surface water features in meters\n",
    "> * `Horizontal_Distance_To_Roadways`: Horizontal dist to nearest roadway in meters\n",
    "> * `Hillshade_9am`: Hillshade index at 9am, summer solstice\n",
    "> * `Hillshade_Noon`: Hillshade index at noon, summer solstice\n",
    "> * `Hillshade_3pm`: Hillshade index at 3pm, summer solstice\n",
    "> * `Horizontal_Distance_To_Fire_Points`: Horizontal dist to nearest wildfire ignition points, meters\n",
    "> * `Wilderness_Area_x`: Wilderness area designation (3 columns)\n",
    "> * `Soil_Type_x`: Soil Type designation (39 columns)\n",
    "> * `Cover_Type`: 1 for cottonwood/willow, 0 for ponderosa pine\n",
    "\n",
    "This is also an imbalanced dataset, since cottonwood/willow trees are relatively rare in this forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Counts\n",
      "0    35754\n",
      "1     2747\n",
      "Name: Cover_Type, dtype: int64\n",
      "\n",
      "Percentages\n",
      "0    0.928651\n",
      "1    0.071349\n",
      "Name: Cover_Type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Run this cell without changes\n",
    "print(\"Raw Counts\")\n",
    "print(df[\"Cover_Type\"].value_counts())\n",
    "print()\n",
    "print(\"Percentages\")\n",
    "print(df[\"Cover_Type\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, a baseline model that always chose the majority class would have an accuracy of over 92%. Therefore we will want to report additional metrics at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Best Model\n",
    "\n",
    "In a previous lab, we used SMOTE to create additional synthetic data, then tuned the hyperparameters of a logistic regression model to get the following final model metrics:\n",
    "\n",
    "* **Log loss:** 0.13031294393913376\n",
    "* **Accuracy:** 0.9456679825472678\n",
    "* **Precision:** 0.6659919028340081\n",
    "* **Recall:** 0.47889374090247455\n",
    "\n",
    "In this lab, you will try to beat those scores using more-complex, nonparametric models.\n",
    "\n",
    "### Modeling\n",
    "\n",
    "Although you may be aware of some additional model algorithms available from scikit-learn, for this lab you will be focusing on two of them: k-nearest neighbors and decision trees. Here are some reminders about these models:\n",
    "\n",
    "#### kNN - [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "\n",
    "This algorithm — unlike linear models or tree-based models — does not emphasize learning the relationship between the features and the target. Instead, for a given test record, it finds the most similar records in the training set and returns an average of their target values.\n",
    "\n",
    "* **Training speed:** Fast. In theory it's just saving the training data for later, although the scikit-learn implementation has some additional logic \"under the hood\" to make prediction faster.\n",
    "* **Prediction speed:** Very slow. The model has to look at every record in the training set to find the k closest to the new record.\n",
    "* **Requires scaling:** Yes. The algorithm to find the nearest records is distance-based, so it matters that distances are all on the same scale.\n",
    "* **Key hyperparameters:** `n_neighbors` (how many nearest neighbors to find; too few neighbors leads to overfitting, too many leads to underfitting), `p` and `metric` (what kind of distance to use in defining \"nearest\" neighbors)\n",
    "\n",
    "#### Decision Trees - [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "\n",
    "Similar to linear models (and unlike kNN), this algorithm emphasizes learning the relationship between the features and the target. However, unlike a linear model that tries to find linear relationships between each of the features and the target, decision trees look for ways to split the data based on features to decrease the entropy of the target in each split.\n",
    "\n",
    "* **Training speed:** Slow. The model is considering splits based on as many as all of the available features, and it can split on the same feature multiple times. This requires exponential computational time that increases based on the number of columns as well as the number of rows.\n",
    "* **Prediction speed:** Medium fast. Producing a prediction with a decision tree means applying several conditional statements, which is slower than something like logistic regression but faster than kNN.\n",
    "* **Requires scaling:** No. This model is not distance-based. You also can use a `LabelEncoder` rather than `OneHotEncoder` for categorical data, since this algorithm doesn't necessarily assume that the distance between `1` and `2` is the same as the distance between `2` and `3`.\n",
    "* **Key hyperparameters:** Many features relating to \"pruning\" the tree. By default they are set so the tree can overfit, and by setting them higher or lower (depending on the hyperparameter) you can reduce overfitting, but too much will lead to underfitting. These are: `max_depth`, `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_features`, `max_leaf_nodes`, and `min_impurity_decrease`. You can also try changing the `criterion` to \"entropy\" or the `splitter` to \"random\" if you want to change the splitting logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "#### 1. Prepare the Data for Modeling\n",
    "\n",
    "#### 2. Build a Baseline kNN Model\n",
    "\n",
    "#### 3. Build Iterative Models to Find the Best kNN Model\n",
    "\n",
    "#### 4. Build a Baseline Decision Tree Model\n",
    "\n",
    "#### 5. Build Iterative Models to Find the Best Decision Tree Model\n",
    "\n",
    "#### 6. Choose and Evaluate an Overall Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the Data for Modeling\n",
    "\n",
    "The target is `Cover_Type`. In the cell below, split `df` into `X` and `y`, then perform a train-test split with `random_state=42` and `stratify=y` to create variables with the standard `X_train`, `X_test`, `y_train`, `y_test` names.\n",
    "\n",
    "Include the relevant imports as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop('Cover_Type',axis=1)\n",
    "y = df['Cover_Type']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instantiate a `StandardScaler`, fit it on `X_train`, and create new variables `X_train_scaled` and `X_test_scaled` containing values transformed with the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code checks that everything is set up correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell without changes\n",
    "\n",
    "# Checking that df was separated into correct X and y\n",
    "assert type(X) == pd.DataFrame and X.shape == (38501, 52)\n",
    "assert type(y) == pd.Series and y.shape == (38501,)\n",
    "\n",
    "# Checking the train-test split\n",
    "assert type(X_train) == pd.DataFrame and X_train.shape == (28875, 52)\n",
    "assert type(X_test) == pd.DataFrame and X_test.shape == (9626, 52)\n",
    "assert type(y_train) == pd.Series and y_train.shape == (28875,)\n",
    "assert type(y_test) == pd.Series and y_test.shape == (9626,)\n",
    "\n",
    "# Checking the scaling\n",
    "assert X_train_scaled.shape == X_train.shape\n",
    "assert round(X_train_scaled[0][0], 3) == -0.636\n",
    "assert X_test_scaled.shape == X_test.shape\n",
    "assert round(X_test_scaled[0][0], 3) == -1.370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a Baseline kNN Model\n",
    "\n",
    "Build a scikit-learn kNN model with default hyperparameters. Then use `cross_val_score` with `scoring=\"neg_log_loss\"` to find the mean log loss for this model (passing in `X_train_scaled` and `y_train` to `cross_val_score`). You'll need to find the mean of the cross-validated scores, and negate the value (either put a `-` at the beginning or multiply by `-1`) so that your answer is a log loss rather than a negative log loss.\n",
    "\n",
    "Call the resulting score `knn_baseline_log_loss`.\n",
    "\n",
    "Your code might take a minute or more to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12964546386734577"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "\n",
    "# Relevant imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Creating the model\n",
    "knn_baseline_model = KNeighborsClassifier()\n",
    "\n",
    "# Perform cross-validation\n",
    "knn_baseline_log_losss = cross_val_score(knn_baseline_model,X_train_scaled,y_train,scoring=\"neg_log_loss\")\n",
    "knn_baseline_log_loss = -(knn_baseline_log_losss.mean())\n",
    "\n",
    "knn_baseline_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best logistic regression model had a log loss of 0.13031294393913376\n",
    "\n",
    "Is this model better? Compare it in terms of metrics and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYes it is better in terms of log loss it has a lower log loss but a bit slow\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "Yes it is better in terms of log loss it has a lower log loss but a bit slow\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Iterative Models to Find the Best kNN Model\n",
    "\n",
    "Build and evaluate at least two more kNN models to find the best one. Explain why you are changing the hyperparameters you are changing as you go. These models will be *slow* to run, so be thinking about what you might try next as you run them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to add the number of neighbors and see if it makes a difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here (add more cells as needed)\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "neighbors = np.arange(1,25,2)\n",
    "log_loss = []\n",
    "for n in neighbors:\n",
    "    # Creating the model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=n)\n",
    "    # Perform cross-validation\n",
    "    knn_model_log_losss = cross_val_score(knn_model,X_train_scaled,y_train,scoring=\"neg_log_loss\")\n",
    "    knn_model_log_loss = -(knn_model_log_losss.mean())\n",
    "    log_loss.append(knn_model_log_loss)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(neighbors,log_loss,label=\"log_loss\")\n",
    "plt.title(\"Log_loss Vs Neighbors\")\n",
    "plt.xlabel('Nearest Neighbors')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above looks like it is still reducing, we can increase the number of neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neighbors</th>\n",
       "      <th>Log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.516782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.181132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.129645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.091885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.081335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.073929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>0.070667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>0.067960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>0.064232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>0.063681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21</td>\n",
       "      <td>0.065415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23</td>\n",
       "      <td>0.064920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Neighbors  Log_loss\n",
       "0           1  0.516782\n",
       "1           3  0.181132\n",
       "2           5  0.129645\n",
       "3           7  0.091885\n",
       "4           9  0.081335\n",
       "5          11  0.073929\n",
       "6          13  0.070667\n",
       "7          15  0.067960\n",
       "8          17  0.064232\n",
       "9          19  0.063681\n",
       "10         21  0.065415\n",
       "11         23  0.064920"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame({'Neighbors':neighbors,'Log_loss':log_loss})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07871799429857204"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here (add more cells as needed)\n",
    "knn_model2 = KNeighborsClassifier(n_neighbors=50)\n",
    "    # Perform cross-validation\n",
    "knn_model2_log_losss = cross_val_score(knn_model2,X_train_scaled,y_train,scoring=\"neg_log_loss\")\n",
    "knn_model2_log_loss = -(knn_model2_log_losss.mean())\n",
    "knn_model2_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one with 50 is worse than the one with 23 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#What if we fit together with a metric \n",
    "neighbors2 = np.arange(1,25,2)\n",
    "log_loss2 = []\n",
    "for n in neighbors2:\n",
    "    # Creating the model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=n,metric=\"manhattan\")\n",
    "    # Perform cross-validation\n",
    "    knn_model_log_losss = cross_val_score(knn_model,X_train_scaled,y_train,scoring=\"neg_log_loss\")\n",
    "    knn_model_log_loss = -(knn_model_log_losss.mean())\n",
    "    log_loss2.append(knn_model_log_loss)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(neighbors2,log_loss2,label=\"log_loss\")\n",
    "plt.title(\"Log_loss Vs Neighbors\")\n",
    "plt.xlabel('Nearest Neighbors')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neighbors</th>\n",
       "      <th>Log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.534257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.169856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.118101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.086497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.064736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>0.062507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>0.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>0.062288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>0.060837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>19</td>\n",
       "      <td>0.061580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21</td>\n",
       "      <td>0.062294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23</td>\n",
       "      <td>0.063943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Neighbors  Log_loss\n",
       "0           1  0.534257\n",
       "1           3  0.169856\n",
       "2           5  0.118101\n",
       "3           7  0.086497\n",
       "4           9  0.064736\n",
       "5          11  0.062507\n",
       "6          13  0.063800\n",
       "7          15  0.062288\n",
       "8          17  0.060837\n",
       "9          19  0.061580\n",
       "10         21  0.062294\n",
       "11         23  0.063943"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2 = pd.DataFrame({'Neighbors':neighbors2,'Log_loss':log_loss2})\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07631568557001107"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model3 = KNeighborsClassifier(n_neighbors=50,metric='manhattan')\n",
    "    # Perform cross-validation\n",
    "knn_model3_log_losss = cross_val_score(knn_model3,X_train_scaled,y_train,scoring=\"neg_log_loss\")\n",
    "knn_model3_log_loss = -(knn_model3_log_losss.mean())\n",
    "knn_model3_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06609085283647362"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model4 = KNeighborsClassifier(n_neighbors=26,metric='manhattan')\n",
    "    # Perform cross-validation\n",
    "knn_model4_log_losss = cross_val_score(knn_model4,X_train_scaled,y_train,scoring=\"neg_log_loss\")\n",
    "knn_model4_log_loss = -(knn_model4_log_losss.mean())\n",
    "knn_model4_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06083682497489382"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model5 = KNeighborsClassifier(n_neighbors=17,metric='manhattan')\n",
    "    # Perform cross-validation\n",
    "knn_model5_log_losss = cross_val_score(knn_model5,X_train_scaled,y_train,scoring=\"neg_log_loss\")\n",
    "knn_model5_log_loss = -(knn_model5_log_losss.mean())\n",
    "knn_model5_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our calculation looks like 17 nearest neighbors is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a Baseline Decision Tree Model\n",
    "\n",
    "Now that you have chosen your best kNN model, start investigating decision tree models. First, build and evaluate a baseline decision tree model, using default hyperparameters (with the exception of `random_state=42` for reproducibility).\n",
    "\n",
    "(Use cross-validated log loss, just like with the previous models.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7352281158853683"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_baseline = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "tree_baseline_loss = -cross_val_score(dt_baseline,X_train,y_train,scoring='neg_log_loss').mean()\n",
    "#tree_baseline_loss_mean = -(tree_baseline_loss.mean())\n",
    "tree_baseline_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret this score. How does this compare to the log loss from our best logistic regression and best kNN models? Any guesses about why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIt is much worse than our KNN model probably because we are overfitting due to no pruning done in our data\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "It is much worse than our KNN model probably because we are overfitting due to no pruning done in our data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Iterative Models to Find the Best Decision Tree Model\n",
    "\n",
    "Build and evaluate at least two more decision tree models to find the best one. Explain why you are changing the hyperparameters you are changing as you go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to change the max_depth, the min_samples_split and min_sample_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max_Depth</th>\n",
       "      <th>Log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.211223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.171105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.149501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.130282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.119342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.114565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.130085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.188153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.269739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.346341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.462152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.536727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.567771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.621900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.654513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.683650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.718724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.729296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.726128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.719639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.735228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Max_Depth  Log_loss\n",
       "0           1  0.211223\n",
       "1           2  0.171105\n",
       "2           3  0.149501\n",
       "3           4  0.130282\n",
       "4           5  0.119342\n",
       "5           6  0.114565\n",
       "6           7  0.130085\n",
       "7           8  0.188153\n",
       "8           9  0.269739\n",
       "9          10  0.346341\n",
       "10         11  0.462152\n",
       "11         12  0.536727\n",
       "12         13  0.567771\n",
       "13         14  0.621900\n",
       "14         15  0.654513\n",
       "15         16  0.683650\n",
       "16         17  0.718724\n",
       "17         18  0.729296\n",
       "18         19  0.726128\n",
       "19         20  0.719639\n",
       "20         21  0.735228\n",
       "21         22  0.735228\n",
       "22         23  0.735228\n",
       "23         24  0.735228\n",
       "24         25  0.735228\n",
       "25         26  0.735228\n",
       "26         27  0.735228\n",
       "27         28  0.735228\n",
       "28         29  0.735228\n",
       "29         30  0.735228\n",
       "30         31  0.735228\n",
       "31         32  0.735228"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here (add more cells as needed)\n",
    "max_depth_values = np.arange(1,33)\n",
    "log_loss_tr = []\n",
    "for depth in max_depth_values:\n",
    "    dt = DecisionTreeClassifier(random_state=42,max_depth=depth)\n",
    "    tree_loss = -cross_val_score(dt,X_train,y_train,scoring='neg_log_loss').mean()\n",
    "    log_loss_tr.append(tree_loss)\n",
    "    \n",
    "results_tr = pd.DataFrame({\"Max_Depth\":max_depth_values, \"Log_loss\":log_loss_tr})\n",
    "results_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best is the one with 6 as max_depth but still worse than our knn model, try adding min_samples_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Min_Split</th>\n",
       "      <th>Log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.157416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.159388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.160052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.160052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.161060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.161060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.163973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.166885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.178611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Min_Split  Log_loss\n",
       "0        0.1  0.157416\n",
       "1        0.2  0.159388\n",
       "2        0.3  0.160052\n",
       "3        0.4  0.160052\n",
       "4        0.5  0.161060\n",
       "5        0.6  0.161060\n",
       "6        0.7  0.163973\n",
       "7        0.8  0.166885\n",
       "8        0.9  0.178611\n",
       "9        1.0  0.211223"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here (add more cells as needed)\n",
    "min_sample_split_values = np.arange(0.1,1.1,0.1)\n",
    "log_loss_tr2 = []\n",
    "for split in min_sample_split_values:\n",
    "    dt2 = DecisionTreeClassifier(random_state=42,max_depth=6,min_samples_split=split)\n",
    "    tree_loss = -cross_val_score(dt2,X_train,y_train,scoring='neg_log_loss').mean()\n",
    "    log_loss_tr2.append(tree_loss)\n",
    "    \n",
    "results_tr2 = pd.DataFrame({\"Min_Split\":min_sample_split_values, \"Log_loss\":log_loss_tr2})\n",
    "results_tr2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least is the one with the min split of 0.1 now let us try adding the min_sample_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Min_Sample_Leaf</th>\n",
       "      <th>Log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.181230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.187993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.218633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.218633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.248986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Min_Sample_Leaf  Log_loss\n",
       "0              0.1  0.181230\n",
       "1              0.2  0.187993\n",
       "2              0.3  0.218633\n",
       "3              0.4  0.218633\n",
       "4              0.5  0.248986"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here (add more cells as needed)\n",
    "min_sample_leaf_values = np.arange(0.1,0.6,0.1)\n",
    "log_loss_tr3 = []\n",
    "for sample in min_sample_leaf_values:\n",
    "    dt3 = DecisionTreeClassifier(random_state=42,max_depth=6,min_samples_split=0.1, min_samples_leaf=sample)\n",
    "    tree_loss = -cross_val_score(dt3,X_train,y_train,scoring='neg_log_loss').mean()\n",
    "    log_loss_tr3.append(tree_loss)\n",
    "    \n",
    "results_tr3 = pd.DataFrame({\"Min_Sample_Leaf\":min_sample_leaf_values, \"Log_loss\":log_loss_tr3})\n",
    "results_tr3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least loss is with min_samples_leaf of 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Choose and Evaluate an Overall Best Model\n",
    "\n",
    "Which model had the best performance? What type of model was it?\n",
    "\n",
    "Instantiate a variable `final_model` using your best model with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(metric=&#x27;manhattan&#x27;, n_neighbors=17)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(metric=&#x27;manhattan&#x27;, n_neighbors=17)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(metric='manhattan', n_neighbors=17)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "final_model = KNeighborsClassifier(n_neighbors=17,metric='manhattan')\n",
    "\n",
    "# Fit the model on the full training data\n",
    "# (scaled or unscaled depending on the model)\n",
    "final_model.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, evaluate the log loss, accuracy, precision, and recall. This code is mostly filled in for you, but you need to replace `None` with either `X_test` or `X_test_scaled` depending on the model you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log loss:  0.06736702363469528\n",
      "accuracy:  0.9780801994597964\n",
      "precision: 0.8993288590604027\n",
      "recall:    0.7802037845705968\n"
     ]
    }
   ],
   "source": [
    "# Replace None with appropriate code\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, log_loss\n",
    "\n",
    "preds = final_model.predict(X_test_scaled)\n",
    "probs = final_model.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"log loss: \", log_loss(y_test, probs))\n",
    "print(\"accuracy: \", accuracy_score(y_test, preds))\n",
    "print(\"precision:\", precision_score(y_test, preds))\n",
    "print(\"recall:   \", recall_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpret your model performance. How would it perform on different kinds of tasks? How much better is it than a \"dummy\" model that always chooses the majority class, or the logistic regression described at the start of the lab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOur model is better than the logistic regression it has a lower log loss and also better than a dummy model that chooses \\nthe majority class.\\nThis is because a dummy model would have an accuracy of 92% and this model has an accuracy of 98%\\nOur precision is 90% meaning that when the model suggest positive it is accurate 90% of the time\\nRecall is 78% is the True positive rate, the model correctly identified 78% of actual positives.\\nThis is better than the 48% of the logistic regression model, but still doesn\\'tinstill a lot of confidence. \\nIf the business really cared about avoiding \"false negatives\" (labeling cottonwood/willow as ponderosa pine) more so than\\navoiding \"false positives\" (labeling ponderosa pine as cottonwood/willow), then we might want to adjust\\nthe decision threshold on this\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace None with appropriate text\n",
    "\"\"\"\n",
    "Our model is better than the logistic regression it has a lower log loss and also better than a dummy model that chooses \n",
    "the majority class.\n",
    "This is because a dummy model would have an accuracy of 92% and this model has an accuracy of 98%\n",
    "Our precision is 90% meaning that when the model suggest positive it is accurate 90% of the time\n",
    "Recall is 78% is the True positive rate, the model correctly identified 78% of actual positives.\n",
    "This is better than the 48% of the logistic regression model, but still doesn't instill a lot of confidence. \n",
    "If the business really cared about avoiding \"false negatives\" (labeling cottonwood/willow as ponderosa pine) more so than\n",
    "avoiding \"false positives\" (labeling ponderosa pine as cottonwood/willow), then we might want to adjust\n",
    "the decision threshold on this\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, you practiced the end-to-end machine learning process with multiple model algorithms, including tuning the hyperparameters for those different algorithms. You saw how nonparametric models can be more flexible than linear models, potentially leading to overfitting but also potentially reducing underfitting by being able to learn non-linear relationships between variables. You also likely saw how there can be a tradeoff between speed and performance, with good metrics correlating with slow speeds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
